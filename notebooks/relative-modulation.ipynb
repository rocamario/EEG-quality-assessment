{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the automated analysis of EEG quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will introduce you to the challenge by going through the data and working towards a first very simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's load the training data\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path(\"../data/train/\")\n",
    "training_data = [(np.load(ROOT_PATH / f\"data_{i}.npy\"),np.load(ROOT_PATH / f\"target_{i}.npy\")) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to have five channels and one label per channel for each two seconds of data.\n",
    "Let's have a look at the data duration and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter the signal to improve the visualisation\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get the point that maps to a label\n",
    "\n",
    "def reshape_array_into_windows(x, sample_rate, window_duration_in_seconds):\n",
    "    \"\"\"\n",
    "    Reshape the data into an array of shape (C, T, window) where 'window' contains\n",
    "    the points corresponding to 'window_duration' seconds of data.\n",
    "\n",
    "    Parameters:\n",
    "    x (numpy array): The input data array.\n",
    "    sample_rate (int): The number of samples per second.\n",
    "    window_duration_in_seconds (float): The duration of each window in seconds.\n",
    "\n",
    "    Returns:\n",
    "    reshaped_x (numpy array): The reshaped array with shape (C, T, window).\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples in one window\n",
    "    window_size = int(window_duration_in_seconds * sample_rate)\n",
    "    \n",
    "    # Ensure the total length of x is a multiple of window_size\n",
    "    total_samples = x.shape[-1]\n",
    "    if total_samples % window_size != 0:\n",
    "        # Truncate or pad x to make it divisible by window_size\n",
    "        x = x[..., :total_samples - (total_samples % window_size)]\n",
    "    # Reshape x into (C, T, window)\n",
    "    reshaped_x = x.reshape(x.shape[0], -1, window_size)\n",
    "\n",
    "    return reshaped_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple model based on our observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load and reshape all the data\n",
    "all_data = []\n",
    "all_targets = []\n",
    "for (data,target) in training_data:\n",
    "    filtered_data =  butter_bandpass_filter(data,0.1,18,250,4)\n",
    "    reshaped_data = reshape_array_into_windows(filtered_data,250,2)\n",
    "    targets_flatten = target[..., :len(reshaped_data[0])].reshape(-1)\n",
    "    reshaped_data = reshaped_data.reshape((-1,reshaped_data.shape[-1]))\n",
    "    all_data.append(reshaped_data)\n",
    "    all_targets.append(targets_flatten)\n",
    "all_data = np.concatenate(all_data)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "assert all_data.shape[0] == all_targets.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now compute the features over each 2 seconds segment\n",
    "from features.time_domain_features import extract_time_domain_features, extract_amplitude_modulation_features\n",
    "from features.frequency_domain_features import extract_frequency_domain_features, extract_relative_power\n",
    "from features.complexity_features import extract_entropy_features\n",
    "from features.wavelet_decomposition import extract_wavelet_energy_features\n",
    "\n",
    "time_features = extract_time_domain_features(all_data, return_type=\"dataframe\")\n",
    "amplitude_modulation_features = extract_amplitude_modulation_features(all_data)\n",
    "frequency_features = extract_frequency_domain_features(all_data)\n",
    "relative_power_features = extract_relative_power(all_data)\n",
    "entropy_features = extract_entropy_features(all_data)\n",
    "wavelet_energy_features = extract_wavelet_energy_features(all_data)\n",
    "\n",
    "# Combine the time, frequency, and entropy features into a single DataFrame\n",
    "features = pd.concat([time_features, amplitude_modulation_features, \n",
    "                      frequency_features, relative_power_features,\n",
    "                      entropy_features, wavelet_energy_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv(\"features/more-features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amplitude', 'mean', 'max', 'min', 'stdev', 'skewness', 'kurtosis',\n",
       "       'hjorth_activity', 'hjorth_mobility', 'hjorth_complexity',\n",
       "       'envelope_mean', 'envelope_std', 'envelope_max', 'envelope_min',\n",
       "       'delta_power', 'theta_power', 'alpha_power', 'beta_power',\n",
       "       'gamma_power', 'delta_relative_power', 'theta_relative_power',\n",
       "       'alpha_relative_power', 'beta_relative_power', 'gamma_relative_power',\n",
       "       'shannon_entropy', 'sample_entropy', 'spectral_entropy',\n",
       "       'energy_band_0', 'energy_band_1', 'energy_band_2', 'energy_band_3',\n",
       "       'energy_band_4', 'energy_band_5', 'energy_band_6', 'energy_band_7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv(\"features/more-features.csv\")\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train a model on 70% of the data and evaluate the model on the remaining 30%\n",
    "prop_train = 0.7\n",
    "n_train = int(prop_train * len(features))\n",
    "\n",
    "x_train = features[:n_train]\n",
    "y_train = all_targets[:n_train]\n",
    "\n",
    "x_val = features[n_train:]\n",
    "y_val = all_targets[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                    \n",
      "Generation 1 - Current best internal CV score: 0.9178162785811159\n",
      "                                                                                      \n",
      "Generation 2 - Current best internal CV score: 0.9178162785811159\n",
      "                                                                                    \n",
      "Generation 3 - Current best internal CV score: 0.9178162785811159\n",
      "                                                                                    \n",
      "Generation 4 - Current best internal CV score: 0.9183358484078623\n",
      "                                                               \n",
      "Generation 5 - Current best internal CV score: 0.91869878321331\n",
      "                                                               \n",
      "Best pipeline: XGBClassifier(CombineDFs(input_matrix, input_matrix), learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, n_jobs=1, subsample=0.1, verbosity=0)\n"
     ]
    }
   ],
   "source": [
    "from models.automl import train_automl_model\n",
    "\n",
    "# Train the AutoML model\n",
    "model = train_automl_model(features, all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen:  0.6903692881768673\n",
      "F1 score:  0.7940495245268807\n"
     ]
    }
   ],
   "source": [
    "from models.automl import evaluate_model\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(\"models/automl_model_all_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the cohen kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you think of the performances ?\n",
    "- What do you think of the split strategy ?\n",
    "- What are additional features you could use ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model on the test data and submitting to the leaderboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features.frequency_domain_features import extract_frequency_domain_features_multichannel\n",
    "from features.time_domain_features import extract_time_domain_features\n",
    "from features.complexity_features import extract_multichannel_entropy_features\n",
    "from features.wavelet_decomposition import extract_wavelet_energy_features_multichannel\n",
    "from features.time_domain_features import extract_amplitude_modulation_features\n",
    "from features.frequency_domain_features import extract_relative_power\n",
    "\n",
    "ROOT_TEST_PATH = Path(\"../data/test/\")\n",
    "test_data = {i:np.load(ROOT_TEST_PATH / f\"data_{i}.npy\") for i in [4,5]}\n",
    "# We process each record independantly\n",
    "\n",
    "def compute_features_on_record(data):\n",
    "    \"\"\"\n",
    "    We compute each of the feature for each window and each channel\n",
    "    Each value of the output dict has shape (Channels,T)\n",
    "    \"\"\"\n",
    "    filtered_data =  butter_bandpass_filter(data,0.1,18,250,4)\n",
    "    reshaped_data = reshape_array_into_windows(filtered_data,250,2)\n",
    "    print(\"Before any feature extraction: \", reshaped_data.shape)\n",
    "    \n",
    "    time_features = extract_time_domain_features(reshaped_data, return_type=\"numpy\")\n",
    "    print(\"Time features shape: \", {k: time_features[k].shape for k in time_features})\n",
    "\n",
    "    amplitude_modulation_features = extract_amplitude_modulation_features(reshaped_data)\n",
    "    print(\"Amplitude modulation features shape: \", {k: amplitude_modulation_features[k].shape for k in amplitude_modulation_features})\n",
    "\n",
    "    frequency_features = extract_frequency_domain_features_multichannel(reshaped_data)\n",
    "    print(\"Frequency features shape: \", {k: frequency_features[k].shape for k in frequency_features})\n",
    "\n",
    "    relative_power_features = extract_relative_power(reshaped_data)\n",
    "    print(\"Relative power features shape: \", {k: relative_power_features[k].shape for k in relative_power_features})\n",
    "\n",
    "    entropy_features = extract_multichannel_entropy_features(reshaped_data)\n",
    "    print(\"Entropy features shape: \", {k: entropy_features[k].shape for k in entropy_features})\n",
    "\n",
    "    wavelet_energy_features_multichannel = extract_wavelet_energy_features_multichannel(reshaped_data)\n",
    "    print(\"Wavelet energy features shape: \", {k: wavelet_energy_features_multichannel[k].shape for k in wavelet_energy_features_multichannel})\n",
    "    \n",
    "    features = {**time_features, **frequency_features, **entropy_features, **wavelet_energy_features_multichannel}\n",
    "    print(\"Features shape: \", {k:features[k].shape for k in features})\n",
    "    \n",
    "    return features  # {5 ch x 13k, 5 ch x 13k, . . .}\n",
    "\n",
    "\n",
    "\n",
    "def compute_predictions_on_record(data,model,features_name_for_model):\n",
    "    predictions = []\n",
    "    features = compute_features_on_record(data)\n",
    "    features = np.array([features[k] for k in features_name_for_model]) \n",
    "    features = features.swapaxes(0,1).swapaxes(1,2)\n",
    "    for channel in range(features.shape[0]):\n",
    "        predictions.append(model.predict(features[channel]))\n",
    "    return np.array(predictions)\n",
    "\n",
    "def format_array_to_target_format(array, record_number):\n",
    "    assert isinstance(record_number, int)\n",
    "    assert isinstance(array, np.ndarray)\n",
    "    assert len(array.shape) == 2\n",
    "    assert array.shape[0] == 5\n",
    "    print(set(np.unique(array)))\n",
    "    assert set(np.unique(array)) == {0, 1}\n",
    "    formatted_target = []\n",
    "    for i in range(array.shape[0]):\n",
    "        channel_encoding = (i + 1) * 100000\n",
    "        record_number_encoding = record_number * 1000000\n",
    "        for j in range(array.shape[1]):\n",
    "            formatted_target.append(\n",
    "                {\n",
    "                    \"identifier\": record_number_encoding + channel_encoding + j,\n",
    "                    \"target\": array[i, j],\n",
    "                }\n",
    "            )\n",
    "    return formatted_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the functions defined above, we can now run the model and submit the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before any feature extraction:  (5, 13204, 500)\n",
      "Time features shape:  {'amplitude': (5, 13204), 'mean': (5, 13204), 'max': (5, 13204), 'min': (5, 13204), 'stdev': (5, 13204), 'skewness': (5, 13204), 'kurtosis': (5, 13204), 'hjorth_activity': (5, 13204), 'hjorth_mobility': (5, 13204), 'hjorth_complexity': (5, 13204)}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Per-column arrays must each be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record_number, data \u001b[38;5;129;01min\u001b[39;00m test_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 5\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_predictions_on_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     formatted_preds \u001b[38;5;241m=\u001b[39m format_array_to_target_format(preds,record_number)\n\u001b[1;32m      7\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(formatted_preds)\n",
      "Cell \u001b[0;32mIn[11], line 48\u001b[0m, in \u001b[0;36mcompute_predictions_on_record\u001b[0;34m(data, model, features_name_for_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_predictions_on_record\u001b[39m(data,model,features_name_for_model):\n\u001b[1;32m     47\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 48\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_features_on_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([features[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m features_name_for_model]) \n\u001b[1;32m     50\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mcompute_features_on_record\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     21\u001b[0m time_features \u001b[38;5;241m=\u001b[39m extract_time_domain_features(reshaped_data, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime features shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, {k: time_features[k]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m time_features})\n\u001b[0;32m---> 24\u001b[0m amplitude_modulation_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_amplitude_modulation_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmplitude modulation features shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, {k: amplitude_modulation_features[k]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m amplitude_modulation_features})\n\u001b[1;32m     27\u001b[0m frequency_features \u001b[38;5;241m=\u001b[39m extract_frequency_domain_features_multichannel(reshaped_data)\n",
      "File \u001b[0;32m~/eeg-signal-quality-analysis-by-beacon-biosignals/notebooks/features/time_domain_features.py:112\u001b[0m, in \u001b[0;36mextract_amplitude_modulation_features\u001b[0;34m(data, return_type)\u001b[0m\n\u001b[1;32m    109\u001b[0m         features[key] \u001b[38;5;241m=\u001b[39m features[key]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/eeg-signal-quality-analysis-by-beacon-biosignals/challenge/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/eeg-signal-quality-analysis-by-beacon-biosignals/challenge/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eeg-signal-quality-analysis-by-beacon-biosignals/challenge/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/eeg-signal-quality-analysis-by-beacon-biosignals/challenge/lib/python3.12/site-packages/pandas/core/internals/construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    662\u001b[0m         raw_lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(val))\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m val\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Per-column arrays must each be 1-dimensional"
     ]
    }
   ],
   "source": [
    "from models.features_list import list_features\n",
    "\n",
    "results = []\n",
    "for record_number, data in test_data.items():\n",
    "    preds = compute_predictions_on_record(data, model, list_features)\n",
    "    formatted_preds = format_array_to_target_format(preds,record_number)\n",
    "    results.extend(formatted_preds)\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"../results/auto-ml-all-data.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112615"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"../results/auto-ml-all-data.csv\")\n",
    "len(\"It needs to be 112615, it's : \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
